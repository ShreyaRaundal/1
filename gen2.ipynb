{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOcUA0DiBm7oYRrO+jhUbwn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShreyaRaundal/1/blob/main/gen2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ofx6TTPsHVTJ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# --- Hyperparameters ---\n",
        "block_size = 8\n",
        "batch_size = 4\n",
        "data_str = \"hello world\"\n",
        "vocab = sorted(list(set(data_str)))\n",
        "vocab_size = len(vocab)\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "epochs = 1000\n",
        "embed_size = 32\n",
        "\n",
        "# --- Tokenizer ---\n",
        "stoi = {ch: i for i, ch in enumerate(vocab)}\n",
        "itos = {i: ch for ch, i in stoi.items()}\n",
        "encode = lambda s: [stoi[c] for c in s]\n",
        "decode = lambda l: ''.join([itos[i] for i in l])\n",
        "\n",
        "# --- Sample tiny dataset ---\n",
        "data = torch.tensor(encode(data_str), dtype=torch.long).to(device)\n",
        "\n",
        "# --- Create dataset ---\n",
        "def get_batch():\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    return x, y\n",
        "\n",
        "# --- Simple LLM ---\n",
        "class TinyLLM(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
        "        self.fc = nn.Linear(embed_size * block_size, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embed(x)  # (B, T, C)\n",
        "        x = x.view(x.size(0), -1)  # flatten (B, T*C)\n",
        "        return self.fc(x)  # (B, vocab_size)\n",
        "\n",
        "# --- Initialize model ---\n",
        "model = TinyLLM().to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "# --- Training loop ---\n",
        "for epoch in range(epochs):\n",
        "    x, y = get_batch()\n",
        "    logits = model(x)\n",
        "    loss = F.cross_entropy(logits, y[:, -1])\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if epoch % 100 == 0:\n",
        "        print(f\"Epoch {epoch}, Loss: {loss.item():.4f}\")\n",
        "\n",
        "# --- Generation ---\n",
        "def generate(model, start='h', max_new_tokens=20):\n",
        "    model.eval()\n",
        "    idx = torch.tensor([encode(start)], dtype=torch.long).to(device)\n",
        "    for _ in range(max_new_tokens):\n",
        "        idx_cond = idx[:, -block_size:]\n",
        "        if idx_cond.shape[1] < block_size:\n",
        "            padding = torch.zeros(1, block_size - idx_cond.shape[1], dtype=torch.long).to(device)\n",
        "            idx_cond = torch.cat([padding, idx_cond], dim=1)\n",
        "        logits = model(idx_cond)\n",
        "        probs = F.softmax(logits, dim=-1)\n",
        "        next_id = torch.multinomial(probs, num_samples=1)\n",
        "        idx = torch.cat((idx, next_id), dim=1)\n",
        "    return decode(idx[0].tolist())\n",
        "\n",
        "# --- Output ---\n",
        "print(\"Generated text:\", generate(model))\n"
      ]
    }
  ]
}